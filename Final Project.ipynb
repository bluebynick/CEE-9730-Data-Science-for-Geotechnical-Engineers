{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluebynick/CEE-9730-Data-Science-for-Geotechnical-Engineers/blob/main/Final%20Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAc3nggri8nL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "outputId": "e7468fb9-e366-4744-f22c-3fd604d6a163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Files exist?\n",
            "75kpa.xlsx True\n",
            "150 kpa.xlsx True\n",
            "final idk.xlsx True\n",
            "idk.xlsx True\n",
            "Train: (20432, 6) Val: (5108, 6) Test: (1428, 6)\n",
            "Training with lr = 0.01\n",
            "Epoch 1/20\n",
            "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3942 - loss: 6797.2432 - val_accuracy: 0.2809 - val_loss: 172.7401\n",
            "Epoch 2/20\n",
            "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5113 - loss: 108.0966 - val_accuracy: 0.6717 - val_loss: 41.8114\n",
            "Epoch 3/20\n",
            "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5616 - loss: 29.7129 - val_accuracy: 0.3780 - val_loss: 18.5229\n",
            "Epoch 4/20\n",
            "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7308 - loss: 2.9112 - val_accuracy: 0.7445 - val_loss: 0.6635\n",
            "Epoch 5/20\n",
            "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7531 - loss: 0.6328 - val_accuracy: 0.7480 - val_loss: 0.6304\n",
            "Epoch 6/20\n",
            "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7534 - loss: 0.6211 - val_accuracy: 0.7555 - val_loss: 0.6266\n",
            "Epoch 7/20\n",
            "\u001b[1m319/319\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.7551 - loss: 0.6179 - val_accuracy: 0.7553 - val_loss: 0.6265\n",
            "Epoch 8/20\n",
            "\u001b[1m141/319\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7580 - loss: 0.6129"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4137328450.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0;31m     h = model.fit(\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 for step, data in zip(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    192\u001b[0m       \u001b[0;31m# NOTE: We do not colocate the deserialization of composite tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;31m# because not all ops are guaranteed to have non-GPU kernels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mfrom_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    272\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m   return _from_tensor_list_helper(\n\u001b[0m\u001b[1;32m    275\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m       tensor_list)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_specs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_spec_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mflat_ret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(spec, value)\u001b[0m\n\u001b[1;32m    273\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m   return _from_tensor_list_helper(\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m       tensor_list)\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m_from_tensor_list\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_tensor_specs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \"\"\"\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__check_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__check_tensor_list\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    593\u001b[0m                        f\"has {len(specs)} items.\")\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m         raise ValueError(f\"Cannot create a {self.value_type.__name__} from the \"\n\u001b[1;32m    597\u001b[0m                          \u001b[0;34mf\"tensor list because item {i} ({tensor_list[i]!r}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, spec_or_tensor)\u001b[0m\n\u001b[1;32m    994\u001b[0m       \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mspec_or_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcompatible\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m     \"\"\"\n\u001b[0;32m--> 996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_or_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mis_subtype_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, spec_or_value)\u001b[0m\n\u001b[1;32m    884\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_or_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     return (isinstance(spec_or_value, (DenseSpec, self.value_type)) and\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_or_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m             self._shape.is_compatible_with(spec_or_value.shape))\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     return self._type_enum in (other.as_datatype_enum,\n\u001b[0;32m--> 214\u001b[0;31m                                other.base_dtype.as_datatype_enum)\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mis_subtype_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTraceType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mbase_dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_INTERN_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbase_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \"\"\"Returns a non-reference `DType` based on this `DType` (for TF1).\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#variables\n",
        "BATCH_SIZE = 64;\n",
        "EPOCHS = 20;\n",
        "N_CLASSES = 3;\n",
        "histories = {}\n",
        "LOW_THRESHOLD = 3.0\n",
        "HIGH_THRESHOLD = 10.0\n",
        "\n",
        "#get the files. i'd tried with zip files but it ended horribly so i restarted lol\n",
        "testA = os.path.join(\"/content/drive/MyDrive/Master's/Nick/75kpa\", \"75kpa.xlsx\")\n",
        "testB = os.path.join(\"/content/drive/MyDrive/Master's/Nick/150kpa - f\", \"150 kpa.xlsx\")\n",
        "testC = os.path.join(\"/content/drive/MyDrive/Master's/Nick/test 3\", \"final idk.xlsx\")\n",
        "testD = os.path.join(\"/content/drive/MyDrive/Master's/Nick/test 4\", \"idk.xlsx\")\n",
        "print(\"Files exist?\")\n",
        "for f in [testA, testB, testC, testD]:\n",
        "    print(os.path.basename(f), os.path.exists(f))\n",
        "\n",
        "#the pari____ng method\n",
        "#\n",
        "SHEAR_SHEET_NAME = \"1|4|1\"   # shear stage sheet name in your Excel\n",
        "\n",
        "def load_shear_data(path, test_label):\n",
        "    \"\"\" #OMG MULTILINE COMMENTS HOLY F\n",
        "    path: path to Excel file\n",
        "    test_label: \"A\", \"B\", \"C\", or \"D\"\n",
        "    returns: DataFrame with features + axial strain\n",
        "    \"\"\"\n",
        "    raw = pd.read_excel(path, sheet_name=SHEAR_SHEET_NAME)\n",
        "\n",
        "    # find the header row where \"Deviator Stress\" appears\n",
        "    mask = raw.apply(\n",
        "        lambda row: row.astype(str).str.contains(\"Deviator Stress\", case=False, na=False).any(),\n",
        "        axis=1\n",
        "    )\n",
        "    header_row = raw[mask].index[-1]  # last occurrence\n",
        "    header = raw.iloc[header_row].tolist()\n",
        "    header = [h if isinstance(h, str) and h == h else f\"col_{i}\"\n",
        "              for i, h in enumerate(header)]\n",
        "\n",
        "    data = raw.iloc[header_row+1:].reset_index(drop=True)\n",
        "    data.columns = header\n",
        "\n",
        "    # keep the columns we care about\n",
        "    cols = [\n",
        "        \"Time\",\n",
        "        \"Corrected Deviator Stress\",\n",
        "        \"Axial Strain\",\n",
        "        \"Volumetric Strain\",\n",
        "        \"Minor Effective Stress\",\n",
        "        \"Major Effective Stress\",\n",
        "        \"Effective Stress Ratio\",\n",
        "        \"Effective Normal Stress\",\n",
        "    ]\n",
        "    cols = [c for c in cols if c in data.columns]  # handle missing extras\n",
        "    data = data[cols].copy()\n",
        "\n",
        "    data[\"Test\"] = test_label\n",
        "    return data\n",
        "\n",
        "def batch_gen(X, y, batch_size=BATCH_SIZE):\n",
        "    n = X.shape[0]\n",
        "    i = 0\n",
        "    while True:\n",
        "        idx = np.arange(i, i + batch_size) % n\n",
        "        batch_x = X[idx]\n",
        "        batch_y = y[idx]\n",
        "        i += batch_size\n",
        "        yield batch_x, batch_y\n",
        "\n",
        "\n",
        "def build_unet(input_shape, num_classes=N_CLASSES):\n",
        "    \"\"\"\n",
        "    Small MLP classifier. Keeping the name 'build_unet' so the rest\n",
        "    of the training loop doesn't need to change.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# 2. Make a risk label from Axial Strain\n",
        "def strain_to_risk(ea):\n",
        "    ea = float(ea)\n",
        "    if ea < LOW_THRESHOLD:\n",
        "        return 0   # low\n",
        "    elif ea < HIGH_THRESHOLD:\n",
        "        return 1   # moderate\n",
        "    else:\n",
        "        return 2   # high\n",
        "\n",
        "#let's run this bad boy\n",
        "#pass the different folders we have to the pairing method. idk if i should pass test / train_extra too tho\n",
        "# 1. Load four tests into one DataFrame\n",
        "df_A = load_shear_data(testA, \"A\")\n",
        "df_B = load_shear_data(testB, \"B\")\n",
        "df_C = load_shear_data(testC, \"C\")\n",
        "df_D = load_shear_data(testD, \"D\")\n",
        "\n",
        "dfs = [df_A, df_B, df_C, df_D]\n",
        "\n",
        "for df in dfs:\n",
        "    df[\"risk\"] = pd.to_numeric(df[\"Axial Strain\"], errors=\"coerce\").apply(strain_to_risk)\n",
        "\n",
        "all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# 3. Train/val = A,B,C ; Test = D\n",
        "train_df = all_df[all_df[\"Test\"].isin([\"D\",\"B\",\"C\"])].copy()\n",
        "test_df  = all_df[all_df[\"Test\"] == \"A\"].copy()\n",
        "\n",
        "feature_cols = [c for c in train_df.columns if c not in [\"Axial Strain\", \"Volumetric Strain\", \"risk\", \"Test\"]]\n",
        "\n",
        "X = train_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "y = train_df[\"risk\"].to_numpy().astype(\"int32\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "X_test = test_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "y_test = test_df[\"risk\"].to_numpy().astype(\"int32\")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "# 4. Generators\n",
        "train_gen = batch_gen(X_train, y_train, BATCH_SIZE)\n",
        "val_gen   = batch_gen(X_val, y_val, BATCH_SIZE)\n",
        "\n",
        "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
        "val_steps       = max(1, len(X_val) // BATCH_SIZE)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#pass the # of classes to build the model, for each each training rate\n",
        "for lr in [1e-2, 1e-3, 1e-4]:\n",
        "    print(\"Training with lr =\", lr)\n",
        "    model = build_unet(input_shape=(X_train.shape[1],), num_classes=N_CLASSES)\n",
        "    opt = tf.keras.optimizers.Adam(lr)\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    h = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_steps=val_steps,\n",
        "        epochs=EPOCHS,\n",
        "        verbose=1\n",
        "    )\n",
        "    histories[lr] = h.history\n",
        "\n",
        "#plot the training and validation loss curve at each learning rate\n",
        "plt.figure(figsize=(10,6))\n",
        "for lr, hist in histories.items():\n",
        "    plt.plot(hist['loss'], label=f\"train lr={lr}\")\n",
        "    plt.plot(hist['val_loss'], '--', label=f\"val lr={lr}\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss per learning rate\"); plt.legend(); plt.grid(True); plt.figure(figsize=(10,6))\n",
        "plt.show()\n",
        "\n",
        "for lr, hist in histories.items():\n",
        "    plt.plot(hist['loss'], label=f\"train lr={lr}\")\n",
        "    plt.plot(hist['val_loss'], '--', label=f\"val lr={lr}\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss per learning rate\"); plt.legend(); plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "#How to actually test it (numbers time)\n",
        "# 1. Train one \"final\" model at the best LR\n",
        "best_lr = 1e-3  # or whichever looked best in your loss plot\n",
        "\n",
        "model = build_unet(input_shape=(X_train.shape[1],), num_classes=N_CLASSES)\n",
        "opt = tf.keras.optimizers.Adam(best_lr)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = model.fit(\n",
        "    batch_gen(X_train, y_train, BATCH_SIZE),\n",
        "    validation_data=batch_gen(X_val, y_val, BATCH_SIZE),\n",
        "    steps_per_epoch=len(X_train)//BATCH_SIZE,\n",
        "    validation_steps=max(1, len(X_val)//BATCH_SIZE),\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 2. Evaluate on validation + test sets *properly*\n",
        "\n",
        "\n",
        "# validation set\n",
        "val_probs = model.predict(X_val)\n",
        "val_pred = np.argmax(val_probs, axis=1)\n",
        "\n",
        "print(\"Validation performance:\")\n",
        "print(classification_report(\n",
        "    y_val, val_pred,\n",
        "    target_names=[\"low\", \"moderate\", \"high\"]\n",
        "))\n",
        "print(\"Confusion matrix (val):\")\n",
        "print(confusion_matrix(y_val, val_pred))\n",
        "\n",
        "# test set (this is your \"unknown\" Test D)\n",
        "test_probs = model.predict(X_test)\n",
        "test_pred = np.argmax(test_probs, axis=1)\n",
        "\n",
        "print(\"\\nTest performance on weird Test D:\")\n",
        "print(classification_report(\n",
        "    y_test, test_pred,\n",
        "    target_names=[\"low\", \"moderate\", \"high\"]\n",
        "))\n",
        "print(\"Confusion matrix (test):\")\n",
        "print(confusion_matrix(y_test, test_pred))\n",
        "\n",
        "\n",
        "\n",
        "#A visual “proof” that feels satisfying\n",
        "# and feature_cols / X_test / y_test defined like before\n",
        "\n",
        "test_probs = model.predict(X_test)\n",
        "test_pred = np.argmax(test_probs, axis=1)  # 0=low,1=mod,2=high\n",
        "\n",
        "axial_strain = test_df[\"Axial Strain\"].to_numpy().astype(float)\n",
        "deviator_stress = test_df[\"Corrected Deviator Stress\"].to_numpy().astype(float)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "scatter = plt.scatter(\n",
        "    axial_strain,\n",
        "    deviator_stress,\n",
        "    c=test_pred,\n",
        "    cmap=\"viridis\",\n",
        "    s=8\n",
        ")\n",
        "plt.xlabel(\"Axial Strain (%)\")\n",
        "plt.ylabel(\"Deviator Stress (kPa)\")\n",
        "plt.title(\"Model-predicted risk along the stress–strain curve (Test D)\")\n",
        "plt.colorbar(scatter, ticks=[0,1,2], label=\"Risk level\")\n",
        "plt.clim(-0.5, 2.5)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#numbers\n",
        "print(\"Validation performance:\")\n",
        "print(classification_report(y_val, val_pred, target_names=[\"low\",\"moderate\",\"high\"]))\n",
        "print(confusion_matrix(y_val, val_pred))\n",
        "\n",
        "print(\"Test D performance:\")\n",
        "print(classification_report(y_test, test_pred, target_names=[\"low\",\"moderate\",\"high\"]))\n",
        "print(confusion_matrix(y_test, test_pred))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07ZbhrpFtk3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}