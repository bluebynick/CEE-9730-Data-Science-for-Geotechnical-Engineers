{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bluebynick/CEE-9730-Data-Science-for-Geotechnical-Engineers/blob/main/Final%20Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rAc3nggri8nL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "a762270f-2c7d-4eae-aa7f-59e4df8aba2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Files exist?\n",
            "75kpa.xlsx True\n",
            "150 kpa.xlsx True\n",
            "final idk.xlsx True\n",
            "idk.xlsx False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: \"/content/drive/MyDrive/triaxial_data/Master's/Nick/test 4/idk.xlsx\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2742420660.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mdf_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shear_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mdf_C\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shear_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mdf_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_shear_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_D\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2742420660.py\u001b[0m in \u001b[0;36mload_shear_data\u001b[0;34m(path, test_label)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maxial\u001b[0m \u001b[0mstrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSHEAR_SHEET_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# find the header row where \"Deviator Stress\" appears\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1548\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1551\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1403\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"/content/drive/MyDrive/triaxial_data/Master's/Nick/test 4/idk.xlsx\""
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "#variables\n",
        "BATCH_SIZE = 64;\n",
        "EPOCHS = 20;\n",
        "N_CLASSES = 3;\n",
        "histories = {}\n",
        "LOW_THRESHOLD = 3.0\n",
        "HIGH_THRESHOLD = 10.0\n",
        "\n",
        "#get the files. i'd tried with zip files but it ended horribly so i restarted lol\n",
        "testA = os.path.join(\"/content/drive/MyDrive/Master's/Nick/75kpa\", \"75kpa.xlsx\")\n",
        "testB = os.path.join(\"/content/drive/MyDrive/Master's/Nick/150kpa - f\", \"150 kpa.xlsx\")\n",
        "testC = os.path.join(\"/content/drive/MyDrive/Master's/Nick/test 3\", \"final idk.xlsx\")\n",
        "testD = os.path.join(\"/content/drive/MyDrive/Master's/Nick/test 4\", \"idk.xlsx\")\n",
        "print(\"Files exist?\")\n",
        "for f in [testA, testB, testC, testD]:\n",
        "    print(os.path.basename(f), os.path.exists(f))\n",
        "\n",
        "#the pari____ng method\n",
        "#\n",
        "SHEAR_SHEET_NAME = \"1|4|1\"   # shear stage sheet name in your Excel\n",
        "\n",
        "def load_shear_data(path, test_label):\n",
        "    \"\"\" #OMG MULTILINE COMMENTS HOLY F\n",
        "    path: path to Excel file\n",
        "    test_label: \"A\", \"B\", \"C\", or \"D\"\n",
        "    returns: DataFrame with features + axial strain\n",
        "    \"\"\"\n",
        "    raw = pd.read_excel(path, sheet_name=SHEAR_SHEET_NAME)\n",
        "\n",
        "    # find the header row where \"Deviator Stress\" appears\n",
        "    mask = raw.apply(\n",
        "        lambda row: row.astype(str).str.contains(\"Deviator Stress\", case=False, na=False).any(),\n",
        "        axis=1\n",
        "    )\n",
        "    header_row = raw[mask].index[-1]  # last occurrence\n",
        "    header = raw.iloc[header_row].tolist()\n",
        "    header = [h if isinstance(h, str) and h == h else f\"col_{i}\"\n",
        "              for i, h in enumerate(header)]\n",
        "\n",
        "    data = raw.iloc[header_row+1:].reset_index(drop=True)\n",
        "    data.columns = header\n",
        "\n",
        "    # keep the columns we care about\n",
        "    cols = [\n",
        "        \"Time\",\n",
        "        \"Corrected Deviator Stress\",\n",
        "        \"Axial Strain\",\n",
        "        \"Volumetric Strain\",\n",
        "        \"Minor Effective Stress\",\n",
        "        \"Major Effective Stress\",\n",
        "        \"Effective Stress Ratio\",\n",
        "        \"Effective Normal Stress\",\n",
        "    ]\n",
        "    cols = [c for c in cols if c in data.columns]  # handle missing extras\n",
        "    data = data[cols].copy()\n",
        "\n",
        "    data[\"Test\"] = test_label\n",
        "    return data\n",
        "\n",
        "#loading images, resizing them per the nearest neighbour (masks), then remapping the labels,\n",
        "def batch_gen(pairs, batch_size=BATCH_SIZE):\n",
        "  \"\"\"\n",
        "   Simple generator that yields batches from arrays X (features) and y (labels).\n",
        "  \"\"\"\n",
        "  n = X.shape[0]\n",
        "  i = 0\n",
        "  while True:\n",
        "      idx = np.arange(i, i + batch_size) % n\n",
        "      batch_x = X[idx]\n",
        "      batch_y = y[idx]\n",
        "      i += batch_size\n",
        "      yield batch_x, batch_y\n",
        "\n",
        "\n",
        "def build_unet(input_shape, num_classes=N_CLASSES):\n",
        "    \"\"\"\n",
        "    Small MLP classifier. Keeping the name 'build_unet' so the rest\n",
        "    of the training loop doesn't need to change.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "#let's run this bad boy\n",
        "#pass the different folders we have to the pairing method. idk if i should pass test / train_extra too tho\n",
        "# 1. Load four tests into one DataFrame\n",
        "df_A = load_shear_data(testA, \"A\")\n",
        "df_B = load_shear_data(testB, \"B\")\n",
        "df_C = load_shear_data(testC, \"C\")\n",
        "df_D = load_shear_data(testD, \"D\")\n",
        "\n",
        "dfs = [df_A, df_B, df_C, df_D]\n",
        "\n",
        "# 2. Make a risk label from Axial Strain\n",
        "def strain_to_risk(ea):\n",
        "    ea = float(ea)\n",
        "    if ea < LOW_THRESHOLD:\n",
        "        return 0   # low\n",
        "    elif ea < HIGH_THRESHOLD:\n",
        "        return 1   # moderate\n",
        "    else:\n",
        "        return 2   # high\n",
        "\n",
        "for df in dfs:\n",
        "    df[\"risk\"] = pd.to_numeric(df[\"Axial Strain\"], errors=\"coerce\").apply(strain_to_risk)\n",
        "\n",
        "all_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# 3. Train/val = A,B,C ; Test = D\n",
        "train_df = all_df[all_df[\"Test\"].isin([\"A\",\"B\",\"C\"])].copy()\n",
        "test_df  = all_df[all_df[\"Test\"] == \"D\"].copy()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_cols = [c for c in train_df.columns if c not in [\"Axial Strain\", \"Volumetric Strain\", \"risk\", \"Test\"]]\n",
        "\n",
        "X = train_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "y = train_df[\"risk\"].to_numpy().astype(\"int32\")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, shuffle=True, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "X_test = test_df[feature_cols].to_numpy().astype(\"float32\")\n",
        "y_test = test_df[\"risk\"].to_numpy().astype(\"int32\")\n",
        "\n",
        "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "# 4. Generators\n",
        "train_gen = batch_gen(X_train, y_train, batch_size=BATCH_SIZE)\n",
        "val_gen   = batch_gen(X_val, y_val, batch_size=BATCH_SIZE)\n",
        "\n",
        "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
        "val_steps       = max(1, len(X_val) // BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pass the # of classes to build the model, for each each training rate\n",
        "for lr in [1e-2, 1e-3, 1e-4]:\n",
        "    print(\"Training with lr =\", lr)\n",
        "    model = build_unet(input_shape=(X_train.shape[1],), num_classes=N_CLASSES)\n",
        "    opt = tf.keras.optimizers.Adam(lr)\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    h = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_steps=val_steps,\n",
        "        epochs=EPOCHS,\n",
        "        verbose=1\n",
        "    )\n",
        "    histories[lr] = h.history\n",
        "\n",
        "#plot the training and validation loss curve at each learning rate\n",
        "plt.figure(figsize=(10,6))\n",
        "for lr, hist in histories.items():\n",
        "    plt.plot(hist['loss'], label=f\"train lr={lr}\")\n",
        "    plt.plot(hist['val_loss'], '--', label=f\"val lr={lr}\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss per learning rate\"); plt.legend(); plt.grid(True); plt.figure(figsize=(10,6))\n",
        "plt.show()\n",
        "\n",
        "for lr, hist in histories.items():\n",
        "    plt.plot(hist['loss'], label=f\"train lr={lr}\")\n",
        "    plt.plot(hist['val_loss'], '--', label=f\"val lr={lr}\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss per learning rate\"); plt.legend(); plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "07ZbhrpFtk3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}